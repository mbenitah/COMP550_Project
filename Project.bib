%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Michael Benitah at 2019-12-11 22:13:34 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@article{devlin2018bert,
	Author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	Journal = {arXiv preprint arXiv:1810.04805},
	Title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	Year = {2018}}

@misc{BERT,
	Author = {Jacob Devlin and Ming-Wei Chang},
	Date-Added = {2019-12-08 13:15:16 -0500},
	Date-Modified = {2019-12-08 13:20:33 -0500},
	Keywords = {NLP; transformers; BERT},
	Lastchecked = {2019-12-08},
	Month = {11},
	Title = {Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing},
	Url = {https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html},
	Year = {2018},
	Bdsk-Url-1 = {https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html}}

@article{Wolf2019HuggingFacesTS,
	Author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'e}mi Louf and Morgan Funtowicz and Jamie Brew},
	Journal = {ArXiv},
	Title = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
	Volume = {abs/1910.03771},
	Year = {2019}}

@misc{vaswani2017attention,
	Archiveprefix = {arXiv},
	Author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	Eprint = {1706.03762},
	Primaryclass = {cs.CL},
	Title = {Attention Is All You Need},
	Year = {2017}}

@misc{merity2016pointer,
	Archiveprefix = {arXiv},
	Author = {Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
	Eprint = {1609.07843},
	Primaryclass = {cs.CL},
	Title = {Pointer Sentinel Mixture Models},
	Year = {2016}}

@misc{dernoncourt2017pubmed,
	Archiveprefix = {arXiv},
	Author = {Franck Dernoncourt and Ji Young Lee},
	Eprint = {1710.06071},
	Primaryclass = {cs.CL},
	Title = {PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts},
	Year = {2017}}

@misc{peters2018deep,
	Archiveprefix = {arXiv},
	Author = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
	Eprint = {1802.05365},
	Primaryclass = {cs.CL},
	Title = {Deep contextualized word representations},
	Year = {2018}}

@misc{mikolov2013efficient,
	Archiveprefix = {arXiv},
	Author = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
	Eprint = {1301.3781},
	Primaryclass = {cs.CL},
	Title = {Efficient Estimation of Word Representations in Vector Space},
	Year = {2013}}

@inproceedings{pennington2014glove,
	Author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	Booktitle = {Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
	Pages = {1532--1543},
	Title = {Glove: Global vectors for word representation},
	Year = {2014}}

@misc{dai2015semisupervised,
	Archiveprefix = {arXiv},
	Author = {Andrew M. Dai and Quoc V. Le},
	Eprint = {1511.01432},
	Primaryclass = {cs.LG},
	Title = {Semi-supervised Sequence Learning},
	Year = {2015}}

@misc{radford2018improving,
	Author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	Date-Modified = {2019-12-11 22:13:12 -0500},
	Title = {Improving language understanding by generative pre-training},
	Year = {2018}}
